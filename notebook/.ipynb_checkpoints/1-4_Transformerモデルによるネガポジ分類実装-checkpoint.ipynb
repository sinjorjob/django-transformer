{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Transformerモデルによるネガポジ分類実装\n",
    "\n",
    "- 本ファイルでは、クラス分類のTransformerモデルを実装する。\n",
    "- コードは書籍「つくりながら学ぶ! PyTorchによる発展ディープラーニング」を参考に作成。\n",
    "\n",
    "https://github.com/YutaroOgawa/pytorch_advanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※　Ubuntuでの動作を前提としています"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup seeds\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    '''idで示されている単語をベクトルに変換'''\n",
    "\n",
    "    def __init__(self, text_embedding_vectors):\n",
    "        super(Embedder, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding.from_pretrained(\n",
    "            embeddings=text_embedding_vectors, freeze=True)\n",
    "        # freeze=Trueによりバックプロパゲーションで更新されず変化しなくなる。\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_vec = self.embeddings(x)\n",
    "\n",
    "        return x_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認\n",
    "\n",
    "# 前節のDataLoaderなどを取得\n",
    "from utils.dataloader import get_chABSA_DataLoaders_and_TEXT\n",
    "train_dl, val_dl, TEXT = get_chABSA_DataLoaders_and_TEXT(max_length=256, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([996, 300])\n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f7135288908>>, {'<unk>': 0, '<pad>': 1, '<cls>': 2, '<eos>': 3, '0': 4, '、': 5, 'の': 6, 'は': 7, 'た': 8, 'まし': 9, '円': 10, 'に': 11, 'と': 12, '万': 13, 'が': 14, '百': 15, 'し': 16, '％': 17, '．': 18, '（': 19, '）': 20, '億': 21, 'なり': 22, 'を': 23, 'で': 24, '年度': 25, '売上高': 26, '連結会計': 27, 'て': 28, '，': 29, 'により': 30, '増': 31, '比': 32, '減': 33, 'や': 34, '増加': 35, '前期比': 36, 'な': 37, '前': 38, '前年同期': 39, '減少': 40, 'など': 41, '等': 42, '推移': 43, 'こと': 44, '販売': 45, '・': 46, '当': 47, 'も': 48, '事業': 49, '結果': 50, '営業利益': 51, 'する': 52, '千': 53, '影響': 54, 'における': 55, '同': 56, '比べ': 57, '改善': 58, '売上': 59, 'ものの': 60, '利益': 61, '向け': 62, 'から': 63, '経済': 64, '回復': 65, 'による': 66, '前期': 67, 'セグメント': 68, 'また': 69, '需要': 70, 'つき': 71, 'において': 72, '「': 73, '」': 74, '受注': 75, 'いたし': 76, '関連': 77, 'おり': 78, 'ます': 79, 'この': 80, '用': 81, 'あり': 82, '帰属': 83, '堅調': 84, '市場': 85, '親会社': 86, '緩やか': 87, '状況': 88, '環境': 89, '株主': 90, '高': 91, '当社': 92, '経常利益': 93, '及び': 94, '国内': 95, 'なっ': 96, '当期純利益': 97, '収益': 98, '製品': 99, '減収': 100, '工事': 101, '基調': 102, '全体': 103, '海外': 104, '部門': 105, '中国': 106, '損失': 107, 'について': 108, '一方': 109, '中心': 110, '好調': 111, '米国': 112, 'ある': 113, '雇用': 114, 'におきまして': 115, 'として': 116, 'られ': 117, '加え': 118, 'グループ': 119, '伴う': 120, '数量': 121, 'わが国': 122, '分野': 123, 'さ': 124, '景気': 125, '企業': 126, '的': 127, 'あっ': 128, '個人消費': 129, '業績': 130, '増収': 131, '見': 132, '面': 133, '月': 134, '以上': 135, '価格': 136, '業界': 137, '続い': 138, '計上': 139, '上回り': 140, '営業': 141, '続き': 142, '商品': 143, '受け': 144, 'その': 145, 'よう': 146, '動き': 147, 'へ': 148, '中': 149, '効果': 150, '所得': 151, '増益': 152, '持ち直し': 153, '機器': 154, '低迷': 155, '厳しい': 156, '為替': 157, 'ため': 158, '前年度': 159, '拡大': 160, '設備投資': 161, 'とも': 162, '先行き': 163, 'その他': 164, '円高': 165, '背景': 166, '自動車': 167, '欧州': 168, '英国': 169, 'れ': 170, '損益': 171, '新興国': 172, '不透明': 173, '生産': 174, '連結': 175, '大きく': 176, '離脱': 177, 'および': 178, '主力': 179, '低下': 180, '大幅': 181, '子会社': 182, '上昇': 183, '強化': 184, 'ｅｕ': 185, '化': 186, '年': 187, '輸出': 188, '順調': 189, '下回り': 190, '情勢': 191, '減益': 192, 'いる': 193, '品': 194, 'アジア': 195, 'システム': 196, '傾向': 197, '前年': 198, '努め': 199, '引き続き': 200, '政策': 201, '継続': 202, 'み': 203, '低調': 204, '向上': 205, '投資': 206, '新規': 207, '住宅': 208, '平成': 209, 'より': 210, '台': 211, '当期': 212, '活動': 213, '費用': 214, '事業年度': 215, 'これら': 216, '材': 217, 'られる': 218, '産業': 219, 'ほか': 220, '下落': 221, '部品': 222, '伸長': 223, '完成': 224, '減速': 225, '製造': 226, '積極': 227, '設備': 228, 'ず': 229, '問題': 230, '営業損益': 231, '大型': 232, '政府': 233, '顧客': 234, 'はじめ': 235, 'スマートフォン': 236, '一部': 237, '依然として': 238, '売却': 239, '新政権': 240, '主要': 241, '量': 242, '伴い': 243, '期': 244, '経常': 245, 'なか': 246, '削減': 247, '実施': 248, '寄与': 249, '経費': 250, 'い': 251, '世界経済': 252, '動向': 253, '変動': 254, '販売価格': 255, 'したこと': 256, 'ベース': 257, '兆': 258, '剤': 259, '北米': 260, '原料': 261, '取引': 262, '建設': 263, '懸念': 264, '装置': 265, 'しかしながら': 266, '展開': 267, '市況': 268, '維持': 269, '『': 270, '』': 271, 'だ': 272, 'コスト': 273, '不動産': 274, '底堅く': 275, '成長': 276, '経営環境': 277, '総じて': 278, '高まり': 279, 'でき': 280, 'とともに': 281, '加工': 282, '増収増益': 283, '推進': 284, '案件': 285, '食品': 286, 'これ': 287, '半導体': 288, '後半': 289, '機': 290, '機械': 291, '競争': 292, '下回る': 293, '伸ばし': 294, '在庫': 295, '対応': 296, '更新': 297, '材料': 298, '機能': 299, '注力': 300, '激化': 301, '確保': 302, '輸入': 303, '類': 304, 'でし': 305, 'ない': 306, '原材料': 307, '取り組み': 308, '四半期': 309, '拡販': 310, '益': 311, '第': 312, '経済政策': 313, '続く': 314, '額': 315, 'れる': 316, 'シリーズ': 317, '上回る': 318, '下': 319, '事業部門': 320, '付加価値': 321, '別': 322, '利益率': 323, '各種': 324, '品目': 325, '数': 326, '日': 327, '業': 328, '水準': 329, '要因': 330, '賃貸': 331, '車': 332, '電力': 333, '△': 334, 'サービス': 335, '不確実性': 336, '不透明感': 337, '係る': 338, '地域': 339, '安定': 340, '当期純損失': 341, '新': 342, '末': 343, '民間': 344, '現地': 345, '総': 346, '行っ': 347, '鈍化': 348, '関係': 349, '電子部品': 350, 'に対する': 351, '体制': 352, '単価': 353, '取り巻く': 354, '台数': 355, '対策': 356, '店舗': 357, '採算': 358, '消費': 359, '減損損失': 360, '特別損失': 361, '用途': 362, '発売': 363, '調整': 364, '費': 365, '通貨': 366, '進め': 367, 'もの': 368, '並み': 369, '医療': 370, '各': 371, '悪化': 372, '改定': 373, '既存': 374, '業務': 375, '発生': 376, '販売台数': 377, '連結子会社': 378, 'ブランド': 379, '主': 380, '先': 381, '医薬品': 382, '当期利益': 383, '性': 384, '換算': 385, '景気回復': 386, '株式会社': 387, '特別利益': 388, '車載': 389, '遅れ': 390, '高い': 391, '－': 392, 'うち': 393, 'まいり': 394, 'まで': 395, '伸び': 396, '出荷': 397, '原価低減': 398, '変化': 399, '建築': 400, '新型': 401, '日銀': 402, '昨年': 403, '残高': 404, '着工': 405, '薬価': 406, '計画': 407, '資材': 408, '開拓': 409, '高水準': 410, 'もと': 411, 'エレクトロニクス': 412, 'コスト削減': 413, 'ポイント': 414, '不安定': 415, '以降': 416, '企業業績': 417, '伸び悩み': 418, '余': 419, '国内外': 420, '売上台数': 421, '建材': 422, '我が国': 423, '新車': 424, '構成': 425, '為替相場': 426, '特に': 427, '獲得': 428, '率': 429, '群': 430, '船舶': 431, '落ち込み': 432, '開発': 433, '飲料': 434, 'き': 435, 'さらに': 436, 'なお': 437, 'なる': 438, 'ガス': 439, 'ニーズ': 440, 'プラント': 441, '㈱': 442, '下回っ': 443, '下支え': 444, '使用': 445, '出版': 446, '力強': 447, '効率': 448, '原油価格': 449, '合計': 450, '家庭': 451, '工場': 452, '当初': 453, '抑制': 454, '新た': 455, '施策': 456, '樹脂': 457, '比較': 458, '物件': 459, '着実': 460, '若干': 461, '見直し': 462, '販売事業': 463, '資源': 464, '転じ': 465, '運営': 466, '開始': 467, 'デバイス': 468, 'トン': 469, '一般': 470, '世界': 471, '並びに': 472, '中国経済': 473, '価格競争': 474, '公共投資': 475, '取扱': 476, '受託': 477, '固定資産': 478, '大口': 479, '天候不順': 480, '安': 481, '年度末': 482, '所有者': 483, '持続': 484, '料': 485, '景気減速': 486, '本': 487, '節約志向': 488, '繊維': 489, '薬': 490, '薬品': 491, '評価': 492, '販管費': 493, '造船': 494, '連続': 495, '電子': 496, '預金': 497, 'やや': 498, 'ジェネリック医薬品': 499, 'リスク': 500, '取り組ん': 501, '商': 502, '営業活動': 503, '塗料': 504, '実績': 505, '家電': 506, '弱': 507, '政治': 508, '施設': 509, '更に': 510, '比率': 511, '治療': 512, '状態': 513, '販売費及び一般管理費': 514, '軽自動車': 515, '領域': 516, '高騰': 517, 'ａ': 518, 'ｉｔ': 519, '™': 520, 'つつ': 521, 'タイ': 522, '上回っ': 523, '人件費': 524, '今後': 525, '全般': 526, '力': 527, '原価': 528, '型': 529, '売上総利益': 530, '変更': 531, '情報通信': 532, '技術': 533, '投資有価証券': 534, '提案': 535, '日本': 536, '日本経済': 537, '東南アジア': 538, '決定': 539, '法人': 540, '法人税等': 541, '海外市場': 542, '牽引': 543, '百貨店': 544, '石炭': 545, '積極的': 546, '策': 547, '米州': 548, '粗利率': 549, '精密': 550, '系': 551, '自動車部品': 552, '設備工事': 553, '譲渡': 554, '貢献': 555, '販売管理費': 556, '過去最高': 557, '集合住宅': 558, 'ｅ': 559, 'きのこ': 560, 'しかし': 561, 'に対し': 562, 'ほぼ': 563, 'アジア地域': 564, 'フィルム': 565, '一時': 566, '人': 567, '仕入': 568, '他': 569, '促進': 570, '停滞': 571, '円安': 572, '占める': 573, '印刷': 574, '卸売': 575, '反動': 576, '収入': 577, '取得': 578, '含み': 579, '含め': 580, '土木': 581, '基板': 582, '増し': 583, '売上総利益率': 584, '工業': 585, '建設工事': 586, '建設業界': 587, '後': 588, '情報': 589, '成果': 590, '戸建': 591, '拡充': 592, '期間': 593, '構造': 594, '横ばい': 595, '残る': 596, '流通': 597, '生産性': 598, '病院': 599, '目的': 600, '相場': 601, '稼働率': 602, '約': 603, '苦戦': 604, '製': 605, '諸': 606, '負担': 607, '車種': 608, '部材': 609, '銅': 610, '間': 611, '靴': 612, 'お客様': 613, 'こうした': 614, 'ともに': 615, 'にて': 616, 'によって': 617, 'に関する': 618, 'インドネシア': 619, 'ホテル': 620, 'メーカー': 621, 'リニューアル': 622, '上': 623, '上期': 624, '主因': 625, '人手不足': 626, '伸張': 627, '作業': 628, '供給': 629, '保有': 630, '内': 631, '前半': 632, '前年比': 633, '効率化': 634, '包装': 635, '台湾': 636, '合理化': 637, '国内市場': 638, '好転': 639, '始め': 640, '婦人': 641, '容器': 642, '対': 643, '工作機械': 644, '建設機械': 645, '引当金': 646, '当該': 647, '微減': 648, '急速': 649, '手数料': 650, '搭載': 651, '支え': 652, '新製品': 653, '期待': 654, '期末': 655, '欧米': 656, '消費者': 657, '状況下': 658, '登録': 659, '研究開発': 660, '米国経済': 661, '続け': 662, '繰延税金資産': 663, '行う': 664, '衣料': 665, '袋': 666, '貸出金': 667, '輸送': 668, '進ん': 669, '進捗': 670, '金': 671, '金融政策': 672, '鉄': 673, '鋼材': 674, '＋': 675, 'ｌｐ': 676, 'ながら': 677, 'のれん': 678, 'アパレル': 679, 'インド': 680, 'エネルギー': 681, 'シート': 682, 'スポーツ': 683, 'テレビ': 684, 'プリンター': 685, 'ユーザー': 686, 'ロボット': 687, '不安': 688, '並び': 689, '低価格': 690, '依然': 691, '健康': 692, '光学': 693, '全': 694, '出店': 695, '利用': 696, '各社': 697, '合わせ': 698, '含ま': 699, '回路': 700, '固定費': 701, '大規模': 702, '安全': 703, '専門店': 704, '年間': 705, '強い': 706, '当年度': 707, '後発医薬品': 708, '戸数': 709, '投入': 710, '持分法': 711, '支援': 712, '新築': 713, '新設住宅着工戸数': 714, '施工': 715, '有価証券': 716, '本格': 717, '株式': 718, '業務用': 719, '構築': 720, '構造改革': 721, '機能性': 722, '欧州連合': 723, '海外向け': 724, '炭': 725, '為替差損': 726, '燃料': 727, '生産活動': 728, '発電': 729, '社': 730, '納入': 731, '素材': 732, '縮小': 733, '自動車販売': 734, '良': 735, '行い': 736, '衣料品': 737, '販促': 738, '足踏み状態': 739, '進展': 740, '進行': 741, '野菜': 742, '錠': 743, '長期': 744, '電子材料': 745, '電線': 746, '非': 747, '首都圏': 748, '～': 749, '0円': 750, '”': 751, 'せ': 752, 'それぞれ': 753, 'です': 754, 'とどまり': 755, 'なかっ': 756, 'に対して': 757, 'イベント': 758, 'インバウンド': 759, 'カジュアル': 760, 'カー': 761, 'スクラップ': 762, 'スープ': 763, 'ディスプレイ': 764, 'ビジネス': 765, 'フェア': 766, 'ペース': 767, 'マンション': 768, 'マーケット': 769, 'リース': 770, 'レンズ': 771, '一': 772, '一層': 773, '不足': 774, '中国市場': 775, '人員': 776, '他社': 777, '以下': 778, '企画': 779, '伸び悩む': 780, '個人': 781, '債権': 782, '償却': 783, '光': 784, '全般的': 785, '再開発': 786, '加工品': 787, '化学': 788, '医薬': 789, '取組み': 790, '含む': 791, '品質': 792, '器具': 793, '回収': 794, '回復の兆し': 795, '国内需要': 796, '基盤': 797, '報告': 798, '売上原価': 799, '外貨': 800, '外部': 801, '天候': 802, '季節': 803, '官公庁': 804, '導入': 805, '広告': 806, '店': 807, '引き下げ': 808, '微増': 809, '徹底': 810, '応用': 811, '急激': 812, '想定': 813, '戦略': 814, '振るわ': 815, '新設': 816, '新車販売台数': 817, '景況感': 818, '東北': 819, '業態': 820, '概ね': 821, '比較的': 822, '水産物': 823, '減収減益': 824, '物流': 825, '特殊': 826, '生活': 827, '生産量': 828, '用品': 829, '発注': 830, '石油': 831, '石油製品': 832, '社会インフラ': 833, '管理': 834, '紙': 835, '至ら': 836, '記録': 837, '資産': 838, '足踏み': 839, '通信': 840, '通期': 841, '量産': 842, '金属': 843, '金融': 844, '鍋': 845, '関連産業': 846, '電子機器': 847, '電装': 848, '需要拡大': 849, '／': 850, 'ｏｅｍ': 851, '※': 852, 'いずれ': 853, 'こだわり': 854, 'その後': 855, 'つゆ': 856, 'でも': 857, 'ならび': 858, 'ませ': 859, 'まま': 860, 'を通じて': 861, 'ん': 862, 'アイホン': 863, 'エンジン': 864, 'オセアニア': 865, 'カバー': 866, 'ギフト': 867, 'ケア': 868, 'コストダウン': 869, 'コネクタ': 870, 'ゴム': 871, 'ゴルフ': 872, 'シェア': 873, 'ソリューション': 874, 'テキスタイル': 875, 'ハードウェア': 876, 'バイオマス発電': 877, 'パソコン': 878, 'パッケージ': 879, 'ファーマ': 880, 'プラス': 881, 'プリント': 882, 'ポリエチレン': 883, 'マインド': 884, 'メンテナンス': 885, '一段と': 886, '上記': 887, '下期': 888, '中古車': 889, '以外': 890, '件数': 891, '低減': 892, '修理': 893, '兆し': 894, '先進国': 895, '全世界': 896, '円高基調': 897, '再': 898, '冷凍': 899, '処理': 900, '別売': 901, '割合': 902, '功': 903, '加わっ': 904, '加算': 905, '化学品': 906, '原油': 907, '受取': 908, '味': 909, '商事': 910, '営業収益': 911, '図り': 912, '土地': 913, '圧縮': 914, '地元': 915, '地区': 916, '地政学': 917, '大手': 918, '奏し': 919, '宿泊': 920, '対比': 921, '対象': 922, '将来': 923, '小型': 924, '小売': 925, '平均': 926, '店頭': 927, '当たり': 928, '必要': 929, '意識': 930, '感': 931, '戻入': 932, '抗': 933, '拠点': 934, '整備': 935, '日清': 936, '時期': 937, '本数': 938, '染色': 939, '機種': 940, '次': 941, '油圧ショベル': 942, '法人税等調整額': 943, '注': 944, '活用': 945, '活発': 946, '海水淡水化': 947, '浸透': 948, '液体': 949, '液晶': 950, '減損': 951, '減税': 952, '照明': 953, '熊本地震': 954, '版': 955, '現地法人': 956, '生産拠点': 957, '発泡': 958, '発足': 959, '省エネ': 960, '移行': 961, '稼働': 962, '穏やか': 963, '空調': 964, '立上': 965, '競合': 966, '粗利益': 967, '経営成績': 968, '自動': 969, '自動車業界': 970, '自社': 971, '衛生': 972, '補助金': 973, '見込ま': 974, '販売不振': 975, '販売費': 976, '貸家': 977, '追加': 978, '退職': 979, '進み': 980, '適正化': 981, '部分': 982, '量販店': 983, '金利': 984, '金融緩和': 985, '鉄鋼': 986, '鋼': 987, '集約': 988, '電源': 989, '需給': 990, '食': 991, '高まる': 992, 'ｉ': 993, 'ｌｅｄ': 994, 'ｓ': 995})\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors.shape)\n",
    "print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n",
      "入力のテンソルサイズ： torch.Size([8, 256])\n",
      "出力のテンソルサイズ： torch.Size([8, 256, 300])\n"
     ]
    }
   ],
   "source": [
    "# ミニバッチの用意\n",
    "batch = next(iter(train_dl))\n",
    "\"\"\"\n",
    "[torchtext.data.batch.Batch of size 24]\n",
    "\t[.Text]:('[torch.LongTensor of size 24x256]', '[torch.LongTensor of size 24]')\n",
    "\t[.Label]:[torch.LongTensor of size 24]\n",
    "    \n",
    "    \"\"\"\n",
    "print(batch.Text[0].shape)  # 単語ＩＤ\n",
    "\n",
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)  #分散表現をゲット\n",
    "\n",
    "# 入出力\n",
    "x = batch.Text[0]\n",
    "x1 = net1(x)  # 単語をベクトルに\n",
    "\n",
    "print(\"入力のテンソルサイズ：\", x.shape)\n",
    "print(\"出力のテンソルサイズ：\", x1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([996, 300])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    '''入力された単語の位置を示すベクトル情報を付加する'''\n",
    "\n",
    "    def __init__(self, d_model=300, max_seq_len=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model  # 単語ベクトルの次元数\n",
    "\n",
    "        # 単語の順番（pos）と埋め込みベクトルの次元の位置（i）によって一意に定まる値の表をpeとして作成\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        print(\"pe.shape=\", pe.shape)\n",
    "\n",
    "        # GPUが使える場合はGPUへ送る、ここでは省略。実際に学習時には使用する\n",
    "        # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # pe = pe.to(device)\n",
    "\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos /\n",
    "                                          (10000 ** ((2 * (i + 1))/d_model)))\n",
    "\n",
    "        # 表peの先頭に、ミニバッチ次元となる次元を足す\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "        print(\"pe.shape=\", pe.shape)\n",
    "\n",
    "        # 勾配を計算しないようにする\n",
    "        self.pe.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 入力xとPositonal Encodingを足し算する\n",
    "        # xがpeよりも小さいので、大きくする\n",
    "        ret = math.sqrt(self.d_model)*x + self.pe\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe.shape= torch.Size([256, 300])\n",
      "pe.shape= torch.Size([256, 300])\n",
      "入力のテンソルサイズ： torch.Size([8, 256, 300])\n",
      "出力のテンソルサイズ： torch.Size([8, 256, 300])\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "\n",
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "\n",
    "# 入出力\n",
    "x = batch.Text[0]\n",
    "x1 = net1(x)  # 単語をベクトルに\n",
    "x2 = net2(x1)\n",
    "\n",
    "print(\"入力のテンソルサイズ：\", x1.shape)\n",
    "print(\"出力のテンソルサイズ：\", x2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    '''Transformerは本当はマルチヘッドAttentionですが、\n",
    "    分かりやすさを優先しシングルAttentionで実装します'''\n",
    "\n",
    "    def __init__(self, d_model=300):\n",
    "        super().__init__()\n",
    "\n",
    "        # SAGANでは1dConvを使用したが、今回は全結合層で特徴量を変換する\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 出力時に使用する全結合層\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Attentionの大きさ調整の変数\n",
    "        self.d_k = d_model\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # 全結合層で特徴量を変換\n",
    "        k = self.k_linear(k)\n",
    "        q = self.q_linear(q)\n",
    "        v = self.v_linear(v)\n",
    "\n",
    "        # Attentionの値を計算する\n",
    "        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # ここでmaskを計算\n",
    "        mask = mask.unsqueeze(1)\n",
    "        weights = weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # softmaxで規格化をする\n",
    "        normlized_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # AttentionをValueとかけ算\n",
    "        output = torch.matmul(normlized_weights, v)\n",
    "\n",
    "        # 全結合層で特徴量を変換\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, normlized_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n",
    "        '''Attention層から出力を単純に全結合層2つで特徴量を変換するだけのユニットです'''\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)  # torch.Size([24, 256, 1024])\n",
    "        self.dropout = nn.Dropout(dropout)  # torch.Size([24, 256, 1024])\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)  # torch.Size([24, 256,300])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # LayerNormalization層\n",
    "        # https://pytorch.org/docs/stable/nn.html?highlight=layernorm\n",
    "        self.norm_1 = nn.LayerNorm(d_model)   #平均0　標準偏差1に正規化\n",
    "        self.norm_2 = nn.LayerNorm(d_model)   #平均0　標準偏差1に正規化\n",
    "\n",
    "        # Attention層\n",
    "        self.attn = Attention(d_model)\n",
    "\n",
    "        # Attentionのあとの全結合層2つ\n",
    "        self.ff = FeedForward(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 正規化とAttention\n",
    "        x_normlized = self.norm_1(x)\n",
    "        \n",
    "        output, normlized_weights = self.attn(\n",
    "            x_normlized, x_normlized, x_normlized, mask)\n",
    "        \n",
    "        x2 = x + self.dropout_1(output)\n",
    "\n",
    "        # 正規化と全結合層\n",
    "        x_normlized2 = self.norm_2(x2)\n",
    "        output = x2 + self.dropout_2(self.ff(x_normlized2))\n",
    "\n",
    "        return output, normlized_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe.shape= torch.Size([256, 300])\n",
      "pe.shape= torch.Size([256, 300])\n",
      "x.shape= torch.Size([8, 256])\n",
      "入力のテンソルサイズ： torch.Size([8, 256, 300])\n",
      "出力のテンソルサイズ： torch.Size([8, 256, 300])\n",
      "Attentionのサイズ： torch.Size([8, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "\n",
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "net3 = TransformerBlock(d_model=300)\n",
    "\n",
    "# maskの作成\n",
    "x = batch.Text[0]\n",
    "print(\"x.shape=\",x.shape)\n",
    "input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
    "input_mask = (x != input_pad) #padの部分を0、それ以外を1に変換\n",
    "#print(\"x:\",x[0])\n",
    "#print(input_mask[0])\n",
    "\n",
    "# 入出力\n",
    "x1 = net1(x)  # 単語をベクトルに\n",
    "x2 = net2(x1)  # Positon情報を足し算\n",
    "x3, normlized_weights = net3(x2, input_mask)  # Self-Attentionで特徴量を変換\n",
    "\n",
    "print(\"入力のテンソルサイズ：\", x2.shape)\n",
    "print(\"出力のテンソルサイズ：\", x3.shape)\n",
    "print(\"Attentionのサイズ：\", normlized_weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    '''Transformer_Blockの出力を使用し、最後にクラス分類させる'''\n",
    "\n",
    "    def __init__(self, d_model=300, output_dim=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # 全結合層\n",
    "        self.linear = nn.Linear(d_model, output_dim)  # output_dimはポジ・ネガの2つ\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "        nn.init.normal_(self.linear.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = x[:, 0, :]  # 各ミニバッチの各文の先頭の単語の特徴量（300次元）を取り出す\n",
    "        out = self.linear(x0)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe.shape= torch.Size([256, 300])\n",
      "pe.shape= torch.Size([256, 300])\n",
      "入力のテンソルサイズ： torch.Size([8, 256, 300])\n",
      "出力のテンソルサイズ： torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "\n",
    "# ミニバッチの用意\n",
    "batch = next(iter(train_dl))\n",
    "\n",
    "# モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "net3 = TransformerBlock(d_model=300)\n",
    "net4 = ClassificationHead(output_dim=2, d_model=300)\n",
    "\n",
    "# 入出力\n",
    "x = batch.Text[0]\n",
    "x1 = net1(x)  # 単語をベクトルに\n",
    "x2 = net2(x1)  # Positon情報を足し算\n",
    "x3, normlized_weights = net3(x2, input_mask)  # Self-Attentionで特徴量を変換\n",
    "x4 = net4(x3)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n",
    "\n",
    "print(\"入力のテンソルサイズ：\", x3.shape)\n",
    "print(\"出力のテンソルサイズ：\", x4.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終的なTransformerモデルのクラス\n",
    "\n",
    "\n",
    "class TransformerClassification(nn.Module):\n",
    "    '''Transformerでクラス分類させる'''\n",
    "\n",
    "    def __init__(self, text_embedding_vectors, d_model=300, max_seq_len=256, output_dim=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # モデル構築\n",
    "        self.net1 = Embedder(text_embedding_vectors)\n",
    "        self.net2 = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n",
    "        self.net3_1 = TransformerBlock(d_model=d_model)\n",
    "        self.net3_2 = TransformerBlock(d_model=d_model)\n",
    "        self.net4 = ClassificationHead(output_dim=output_dim, d_model=d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x1 = self.net1(x)  # 単語をベクトルに\n",
    "        x2 = self.net2(x1)  # Positon情報を足し算\n",
    "        x3_1, normlized_weights_1 = self.net3_1(\n",
    "            x2, mask)  # Self-Attentionで特徴量を変換\n",
    "        x3_2, normlized_weights_2 = self.net3_2(\n",
    "            x3_1, mask)  # Self-Attentionで特徴量を変換\n",
    "        x4 = self.net4(x3_2)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n",
    "        return x4, normlized_weights_1, normlized_weights_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe.shape= torch.Size([256, 300])\n",
      "pe.shape= torch.Size([256, 300])\n",
      "出力のテンソルサイズ： torch.Size([8, 2])\n",
      "出力テンソルのsigmoid： tensor([[0.7278, 0.2722],\n",
      "        [0.7439, 0.2561],\n",
      "        [0.7556, 0.2444],\n",
      "        [0.7375, 0.2625],\n",
      "        [0.7403, 0.2597],\n",
      "        [0.7303, 0.2697],\n",
      "        [0.7524, 0.2476],\n",
      "        [0.6980, 0.3020]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "\n",
    "# ミニバッチの用意\n",
    "batch = next(iter(train_dl))\n",
    "\n",
    "# モデル構築\n",
    "net = TransformerClassification(\n",
    "    text_embedding_vectors=TEXT.vocab.vectors, d_model=300, max_seq_len=256, output_dim=2)\n",
    "\n",
    "# 入出力\n",
    "x = batch.Text[0]\n",
    "input_mask = (x != input_pad)\n",
    "out, normlized_weights_1, normlized_weights_2 = net(x, input_mask)\n",
    "\n",
    "print(\"出力のテンソルサイズ：\", out.shape)\n",
    "print(\"出力テンソルのsigmoid：\", F.softmax(out, dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
